<!doctype html>
<html lang="en">
<head>
<meta charset="utf-8">
<meta name="viewport" content="width=device-width, initial-scale=1, minimum-scale=1" />
<meta name="generator" content="pdoc 0.10.0" />
<title>tprism.torch_tprism API documentation</title>
<meta name="description" content="" />
<link rel="preload stylesheet" as="style" href="https://cdnjs.cloudflare.com/ajax/libs/10up-sanitize.css/11.0.1/sanitize.min.css" integrity="sha256-PK9q560IAAa6WVRRh76LtCaI8pjTJ2z11v0miyNNjrs=" crossorigin>
<link rel="preload stylesheet" as="style" href="https://cdnjs.cloudflare.com/ajax/libs/10up-sanitize.css/11.0.1/typography.min.css" integrity="sha256-7l/o7C8jubJiy74VsKTidCy1yBkRtiUGbVkYBylBqUg=" crossorigin>
<link rel="stylesheet preload" as="style" href="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/10.1.1/styles/github.min.css" crossorigin>
<style>:root{--highlight-color:#fe9}.flex{display:flex !important}body{line-height:1.5em}#content{padding:20px}#sidebar{padding:30px;overflow:hidden}#sidebar > *:last-child{margin-bottom:2cm}.http-server-breadcrumbs{font-size:130%;margin:0 0 15px 0}#footer{font-size:.75em;padding:5px 30px;border-top:1px solid #ddd;text-align:right}#footer p{margin:0 0 0 1em;display:inline-block}#footer p:last-child{margin-right:30px}h1,h2,h3,h4,h5{font-weight:300}h1{font-size:2.5em;line-height:1.1em}h2{font-size:1.75em;margin:1em 0 .50em 0}h3{font-size:1.4em;margin:25px 0 10px 0}h4{margin:0;font-size:105%}h1:target,h2:target,h3:target,h4:target,h5:target,h6:target{background:var(--highlight-color);padding:.2em 0}a{color:#058;text-decoration:none;transition:color .3s ease-in-out}a:hover{color:#e82}.title code{font-weight:bold}h2[id^="header-"]{margin-top:2em}.ident{color:#900}pre code{background:#f8f8f8;font-size:.8em;line-height:1.4em}code{background:#f2f2f1;padding:1px 4px;overflow-wrap:break-word}h1 code{background:transparent}pre{background:#f8f8f8;border:0;border-top:1px solid #ccc;border-bottom:1px solid #ccc;margin:1em 0;padding:1ex}#http-server-module-list{display:flex;flex-flow:column}#http-server-module-list div{display:flex}#http-server-module-list dt{min-width:10%}#http-server-module-list p{margin-top:0}.toc ul,#index{list-style-type:none;margin:0;padding:0}#index code{background:transparent}#index h3{border-bottom:1px solid #ddd}#index ul{padding:0}#index h4{margin-top:.6em;font-weight:bold}@media (min-width:200ex){#index .two-column{column-count:2}}@media (min-width:300ex){#index .two-column{column-count:3}}dl{margin-bottom:2em}dl dl:last-child{margin-bottom:4em}dd{margin:0 0 1em 3em}#header-classes + dl > dd{margin-bottom:3em}dd dd{margin-left:2em}dd p{margin:10px 0}.name{background:#eee;font-weight:bold;font-size:.85em;padding:5px 10px;display:inline-block;min-width:40%}.name:hover{background:#e0e0e0}dt:target .name{background:var(--highlight-color)}.name > span:first-child{white-space:nowrap}.name.class > span:nth-child(2){margin-left:.4em}.inherited{color:#999;border-left:5px solid #eee;padding-left:1em}.inheritance em{font-style:normal;font-weight:bold}.desc h2{font-weight:400;font-size:1.25em}.desc h3{font-size:1em}.desc dt code{background:inherit}.source summary,.git-link-div{color:#666;text-align:right;font-weight:400;font-size:.8em;text-transform:uppercase}.source summary > *{white-space:nowrap;cursor:pointer}.git-link{color:inherit;margin-left:1em}.source pre{max-height:500px;overflow:auto;margin:0}.source pre code{font-size:12px;overflow:visible}.hlist{list-style:none}.hlist li{display:inline}.hlist li:after{content:',\2002'}.hlist li:last-child:after{content:none}.hlist .hlist{display:inline;padding-left:1em}img{max-width:100%}td{padding:0 .5em}.admonition{padding:.1em .5em;margin-bottom:1em}.admonition-title{font-weight:bold}.admonition.note,.admonition.info,.admonition.important{background:#aef}.admonition.todo,.admonition.versionadded,.admonition.tip,.admonition.hint{background:#dfd}.admonition.warning,.admonition.versionchanged,.admonition.deprecated{background:#fd4}.admonition.error,.admonition.danger,.admonition.caution{background:lightpink}</style>
<style media="screen and (min-width: 700px)">@media screen and (min-width:700px){#sidebar{width:30%;height:100vh;overflow:auto;position:sticky;top:0}#content{width:70%;max-width:100ch;padding:3em 4em;border-left:1px solid #ddd}pre code{font-size:1em}.item .name{font-size:1em}main{display:flex;flex-direction:row-reverse;justify-content:flex-end}.toc ul ul,#index ul{padding-left:1.5em}.toc > ul > li{margin-top:.5em}}</style>
<style media="print">@media print{#sidebar h1{page-break-before:always}.source{display:none}}@media print{*{background:transparent !important;color:#000 !important;box-shadow:none !important;text-shadow:none !important}a[href]:after{content:" (" attr(href) ")";font-size:90%}a[href][title]:after{content:none}abbr[title]:after{content:" (" attr(title) ")"}.ir a:after,a[href^="javascript:"]:after,a[href^="#"]:after{content:""}pre,blockquote{border:1px solid #999;page-break-inside:avoid}thead{display:table-header-group}tr,img{page-break-inside:avoid}img{max-width:100% !important}@page{margin:0.5cm}p,h2,h3{orphans:3;widows:3}h1,h2,h3,h4,h5,h6{page-break-after:avoid}}</style>
<script defer src="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/10.1.1/highlight.min.js" integrity="sha256-Uv3H6lx7dJmRfRvH8TH6kJD1TSK1aFcwgx+mdg3epi8=" crossorigin></script>
<script>window.addEventListener('DOMContentLoaded', () => hljs.initHighlighting())</script>
</head>
<body>
<main>
<article id="content">
<header>
<h1 class="title">Module <code>tprism.torch_tprism</code></h1>
</header>
<section id="section-intro">
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">#!/usr/bin/env python
import torch
import torch.optim as optim
import torch.nn.functional as F
import json
import os
import re
import numpy as np
from google.protobuf import json_format
from itertools import chain
import collections
import argparse
import time
import pickle

import tprism.expl_pb2 as expl_pb2
import tprism.expl_graph as expl_graph
import tprism.torch_expl_graph as torch_expl_graph
import tprism.torch_embedding_generator as embed_gen
from tprism.util import (
    to_string_goal,
    Flags,
    build_goal_dataset,
    split_goal_dataset,
    get_goal_dataset,
)
from tprism.loader import (
    load_input_data,
    load_explanation_graph,
    LossLoader,
)

from tprism.torch_util import draw_graph
import re
import numpy as np
import sklearn.metrics

&#34;&#34;&#34; Main module for command line interface

This is called by tprism command (pytorch based tprism)
&#34;&#34;&#34;

class TprismEvaluator:
    &#34;&#34;&#34;Evaluator for pytorch system

    &#34;&#34;&#34;
    def __init__(self, goal_dataset=None):
        if goal_dataset is not None:
            self.n_goals = len(goal_dataset)
        else:
            self.n_goals = 1
        self.loss_history = [[] for _ in range(self.n_goals)]
        self.loss_dict_history = [[] for _ in range(self.n_goals)]
        self.label = [[] for _ in range(self.n_goals)]
        self.output = [[] for _ in range(self.n_goals)]

    def start_epoch(self):
        self.running_loss = [0.0 for _ in range(self.n_goals)]
        self.running_loss_dict = [{} for _ in range(self.n_goals)]
        self.running_count = [0 for _ in range(self.n_goals)]

    def update(self, loss, loss_dict, j):
        &#34;&#34;&#34;
            This function is called the last of batch iteration
              loss,output, label: return values of forward
              j: goal index of iteration
              num_itr:: number of iterations
        &#34;&#34;&#34;
        self.running_loss[j] += loss
        self.running_count[j] += 1
        for k, v in loss_dict.items():
            if k in self.running_loss_dict:
                self.running_loss_dict[j][k] += v
            else:
                self.running_loss_dict[j][k] = v

    def stop_epoch(self, j=0, mean_flag=True):
        if mean_flag:
            self.running_loss[j] /= self.running_count[j]
            for k in self.running_loss_dict[j].keys():
                self.running_loss_dict[j][k] /= self.running_count[j]
        self.loss_history[j].append(self.running_loss[j])
        self.loss_dict_history[j].append(self.running_loss_dict[j])

    def get_dict(self, prefix=&#34;train&#34;):
        result = {}
        for j in range(self.n_goals):
            key = &#34;{:s}-loss&#34;.format(prefix)
            val = self.running_loss[j]
            result[key] = float(val)
            for k, v in self.running_loss_dict[j].items():
                if k[0] != &#34;*&#34;:
                    m = &#34;{:s}-{:s}-loss&#34;.format(prefix, k)
                else:
                    m = &#34;*{:s}-{:s}&#34;.format(prefix, k[1:])
                result[m] = float(v)
            return result

    def get_msg(self, prefix=&#34;train&#34;):
        msg = []
        for key, val in self.get_dict(prefix=prefix).items():
            m = &#34;{:s}: {:.3f}&#34;.format(key, val)
            msg.append(m)
        return &#34;  &#34;.join(msg)

    def get_loss(self):
        return self.running_loss

    def update_data(self, output, label, j):
        if type(output[j]) == list:  # preference
            _o = [o.detach().numpy() for o in output[j]]
        else:
            _o = output[j].detach().numpy()
        self.output[j].extend(_o)
        if label is not None:
            _l = label[j].detach().numpy()
            self.label[j].extend(_l)


class TprismModel:
    &#34;&#34;&#34;T-PRISM model for pytorch

    &#34;&#34;&#34;
 
    def __init__(self, flags, options, graph, loss_cls):
        self.graph = graph
        self.flags = flags
        self.options = options
        self.loss_cls = loss_cls

    def build(self, input_data, load_embeddings, embedding_key):
        self._build_embedding(embedding_key)
        self._set_data(input_data, load_embeddings)
        self._build_explanation_graph()

    def _build_embedding(self, embedding_key):
        embedding_generators = []
        if self.flags.embedding:
            eg = embed_gen.DatasetEmbeddingGenerator()
            eg.load(self.flags.embedding, key=embedding_key)
            embedding_generators.append(eg)
        if self.flags.const_embedding:
            eg = embed_gen.ConstEmbeddingGenerator()
            eg.load(self.flags.const_embedding)
            embedding_generators.append(eg)
        cycle_embedding_generator = None
        if self.flags.cycle:
            cycle_embedding_generator = embed_gen.CycleEmbeddingGenerator()
            cycle_embedding_generator.load(self.options)
            embedding_generators.append(cycle_embedding_generator)
        self.embedding_generators = embedding_generators
        self.cycle_embedding_generator = cycle_embedding_generator

    def _set_data(self, input_data, load_embeddings):
        self.tensor_provider = torch_expl_graph.TorchSwitchTensorProvider()
        self.tensor_provider.build(
            self.graph,
            self.options,
            input_data,
            self.flags,
            load_embeddings=load_embeddings,
            embedding_generators=self.embedding_generators,
        )

    def _build_explanation_graph(self):
        self.comp_expl_graph = torch_expl_graph.TorchComputationalExplGraph(
            self.graph, self.tensor_provider, self.cycle_embedding_generator
        )

    def solve(self, input_data=None):
        if input_data is None:
            self._solve_no_data()

    def _solve_no_data(self):
        print(&#34;... training phase&#34;)
        print(&#34;... training variables&#34;)
        for key, param in self.comp_expl_graph.state_dict().items():
            print(key, param.shape)
        print(&#34;... building explanation graph&#34;)
        prev_loss = None
        for step in range(self.flags.max_iterate):
            feed_dict = {}
            for embedding_generator in self.embedding_generators:
                if embedding_generator is not None:
                    feed_dict = embedding_generator.build_feed(feed_dict, None)
            self.tensor_provider.set_input(feed_dict)
            # out_inside = self.sess.run(inside, feed_dict=feed_dict)
            goal_inside, loss_list = self.comp_expl_graph.forward(verbose=True)
            inside = []
            for goal in goal_inside:
                l1 = goal[&#34;inside&#34;]
                inside.append(l1)
            loss = 0
            for embedding_generator in self.embedding_generators:
                if embedding_generator is not None:
                    loss = embedding_generator.update(inside)
            print(&#34;step&#34;, step, &#34;loss:&#34;, loss)
            if loss &lt; 1.0e-20:
                break
            if prev_loss is not None and not loss &lt; prev_loss:
                pass
            prev_loss = loss
        ##

    def fit(self, input_data=None, verbose=False):
        if input_data is None:
            return self._fit_no_data(verbose)
        else:
            return self._fit(input_data, verbose)

    def _fit_no_data(self, verbose):
        print(&#34;... training phase&#34;)
        print(&#34;... training variables&#34;)
        for key, param in self.comp_expl_graph.state_dict().items():
            print(key, param.shape)
        optimizer = optim.Adam(
            self.comp_expl_graph.parameters(),
            self.flags.sgd_learning_rate,
            weight_decay=1.0e-10,
        )
        print(&#34;... initialization&#34;)
        loss_cls = self.loss_cls()
        print(&#34;... building explanation graph&#34;)

        best_total_loss = None
        train_evaluator = TprismEvaluator()
        for epoch in range(self.flags.max_iterate):
            start_t = time.time()
            # train
            feed_dict = {}
            for embedding_generator in self.embedding_generators:
                if embedding_generator is not None:
                    feed_dict = embedding_generator.build_feed(feed_dict, None)
            self.tensor_provider.set_input(feed_dict)
            # print(&#34;... iteration&#34;)
            goal_inside, loss_list = self.comp_expl_graph.forward()
            loss, output, label = loss_cls.call(
                self.graph, goal_inside, self.tensor_provider
            )
            optimizer.zero_grad()
            total_loss = torch.sum(loss, dim=0)
            total_loss.backward()
            optimizer.step()
            metrics = loss_cls.metrics(output, label)
            metrics.update(loss_list)
            train_evaluator.start_epoch()
            train_evaluator.update(total_loss, metrics, 0)
            train_evaluator.stop_epoch()
            # display_graph(output[j],&#39;graph_pytorch&#39;)

            # train_acc=sklearn.metrics.accuracy_score(all_label,all_output)
            train_time = time.time() - start_t
            print(&#34;[{:4d}] &#34;.format(epoch + 1), train_evaluator.get_msg(&#34;train&#34;))
            print(&#34;train time:{0}&#34;.format(train_time) + &#34;[sec]&#34;)
            if (
                best_total_loss is None
                or train_evaluator.running_loss[0] &lt; best_total_loss
            ):
                best_total_loss = train_evaluator.running_loss[0]
                self.save(self.flags.model + &#34;.best.model&#34;)
        self.save(self.flags.model + &#34;.last.model&#34;)

    def _build_feed(self, ph_vars, dataset, idx, verbose=False):
        if verbose:
            for i, ph in enumerate(ph_vars):
                print(&#34;[INFO feed]&#34;, ph, ph.name)
                print(&#34;[INFO feed]&#34;, dataset[i, idx].shape)
        feed_dict = {ph: torch.tensor(dataset[i, idx]) for i, ph in enumerate(ph_vars)}
        return feed_dict

    def _set_batch_input(self, goal, train_idx, j, itr):
        batch_size = self.flags.sgd_minibatch_size
        ph_vars = goal[&#34;placeholders&#34;]
        dataset = goal[&#34;dataset&#34;]
        batch_idx = train_idx[j][itr * batch_size : (itr + 1) * batch_size]
        feed_dict = self._build_feed(ph_vars, dataset, batch_idx)
        # for k,v in feed_dict.items():
        #    print(k,v.shape)
        for embedding_generator in self.embedding_generators:
            if embedding_generator is not None:
                feed_dict = embedding_generator.build_feed(feed_dict, batch_idx)
        self.tensor_provider.set_input(feed_dict)

    def _fit(self, input_data, verbose):
        print(&#34;... training phase&#34;)
        goal_dataset = build_goal_dataset(input_data, self.tensor_provider)
        print(&#34;... training variables&#34;)
        for key, param in self.comp_expl_graph.state_dict().items():
            print(key, param.shape)
        optimizer = optim.Adam(
            self.comp_expl_graph.parameters(),
            self.flags.sgd_learning_rate,
            weight_decay=1.0e-10,
        )
        best_valid_loss = [None for _ in range(len(goal_dataset))]
        patient_count = 0
        batch_size = self.flags.sgd_minibatch_size
        print(&#34;... splitting data&#34;)
        train_idx, valid_idx = split_goal_dataset(goal_dataset)
        loss_cls = self.loss_cls()
        print(&#34;... starting training&#34;)
        for epoch in range(self.flags.max_iterate):
            start_t = time.time()
            train_evaluator = TprismEvaluator(goal_dataset)
            valid_evaluator = TprismEvaluator(goal_dataset)
            for j, goal in enumerate(goal_dataset):
                if verbose:
                    print(goal)
                np.random.shuffle(train_idx[j])
                # training update
                num_itr = len(train_idx[j]) // batch_size
                train_evaluator.start_epoch()
                for itr in range(num_itr):
                    self._set_batch_input(goal, train_idx, j, itr)
                    goal_inside, loss_list = self.comp_expl_graph.forward(
                        verbose=verbose
                    )
                    loss, output, label = loss_cls.call(
                        self.graph, goal_inside, self.tensor_provider
                    )
                    if label is not None:
                        metrics = loss_cls.metrics(
                            output[j].detach().numpy(), label[j].detach().numpy()
                        )
                    else:
                        metrics = loss_cls.metrics(output[j].detach().numpy(), None)
                    loss_list.update(metrics)
                    # display_graph(output[j],&#39;graph_pytorch&#39;)
                    optimizer.zero_grad()
                    loss[j].backward()
                    optimizer.step()
                    train_evaluator.update(loss[j], loss_list, j)
                # validation
                num_itr = len(valid_idx[j]) // batch_size
                valid_evaluator.start_epoch()
                for itr in range(num_itr):
                    self._set_batch_input(goal, valid_idx, j, itr)
                    goal_inside, loss_list = self.comp_expl_graph.forward()
                    loss, output, label = loss_cls.call(
                        self.graph, goal_inside, self.tensor_provider
                    )
                    if label is not None:
                        metrics = loss_cls.metrics(
                            output[j].detach().numpy(), label[j].detach().numpy()
                        )
                    else:
                        metrics = loss_cls.metrics(output[j].detach().numpy(), None)
                    loss_list.update(metrics)
                    valid_evaluator.update(loss[j], loss_list, j)
                # checking validation loss for early stopping
                if (
                    best_valid_loss[j] is None
                    or best_valid_loss[j] &gt; valid_evaluator.running_loss[j]
                ):
                    best_valid_loss[j] = valid_evaluator.running_loss[j]
                    patient_count = 0
                    check_point_flag = True
                    self.save(self.flags.model + &#34;.best.model&#34;)
                else:
                    patient_count += 1
                ckpt_msg = &#34;*&#34; if check_point_flag else &#34;&#34;
                print(
                    &#34;[{:4d}] &#34;.format(epoch + 1),
                    train_evaluator.get_msg(&#34;train&#34;),
                    valid_evaluator.get_msg(&#34;valid&#34;),
                    &#34;({:2d})&#34;.format(patient_count),
                    ckpt_msg,
                )
                if patient_count == self.flags.sgd_patience:
                    break
            train_time = time.time() - start_t
            print(&#34;train time:{0}&#34;.format(train_time) + &#34;[sec]&#34;)
        self.save(self.flags.model + &#34;.last.model&#34;)

    def save(self, filename):
        torch.save(self.comp_expl_graph.state_dict(), filename)

    def load(self, filename):
        self.comp_expl_graph.load_state_dict(torch.load(filename))

    def pred(self, input_data=None, verbose=False):
        if input_data is not None:
            return self._pred(input_data, verbose)
        else:
            return self._pred_no_data(verbose)

    def _pred_no_data(self, verbose):
        print(&#34;... prediction&#34;)
        print(&#34;... loaded variables&#34;)
        for key, param in self.comp_expl_graph.state_dict().items():
            print(key, param.shape)
        print(&#34;... initialization&#34;)
        loss_cls = self.loss_cls()
        evaluator = TprismEvaluator()
        ###
        feed_dict = {}
        for embedding_generator in self.embedding_generators:
            if embedding_generator is not None:
                feed_dict = embedding_generator.build_feed(feed_dict, None)
        self.tensor_provider.set_input(feed_dict)
        print(&#34;... predicting&#34;)
        start_t = time.time()
        goal_inside, loss_list = self.comp_expl_graph.forward()
        loss, output, label = loss_cls.call(
            self.graph, goal_inside, self.tensor_provider
        )
        metrics=loss_cls.metrics(output, label)
        # train_acc=sklearn.metrics.accuracy_score(all_label,all_output)
        #print(&#34;loss:&#34;, np.sum(evaluator.get_loss())
        print(&#34;loss:&#34;, torch.sum(loss))
        print(&#34;metrics:&#34;, metrics)
        pred_time = time.time() - start_t
        print(&#34;prediction time:{0}&#34;.format(pred_time) + &#34;[sec]&#34;)
        if label is None:
            label=label.detach().numpy()
        output=output.detach().numpy()
        return label, output

    def export_computational_graph(self, input_data, verbose=False):
        print(&#34;... prediction&#34;)
        goal_dataset = build_goal_dataset(input_data, self.tensor_provider)
        print(&#34;... loaded variables&#34;)
        for key, param in self.comp_expl_graph.state_dict().items():
            print(key, param.shape)
        print(&#34;... initialization&#34;)
        batch_size = self.flags.sgd_minibatch_size
        test_idx = get_goal_dataset(goal_dataset)
        loss_cls = self.loss_cls()
        evaluator = TprismEvaluator(goal_dataset)
        print(&#34;... predicting&#34;)
        start_t = time.time()
        outputs = []
        labels = []
        for j, goal in enumerate(goal_dataset):
            # valid
            num_itr = len(test_idx[j]) // batch_size
            evaluator.start_epoch()
            for itr in range(num_itr):
                self._set_batch_input(goal, test_idx, j, itr)
                goal_inside, loss_list = self.comp_expl_graph.forward(dryrun=True)
                for g in goal_inside:
                    print(g)
                    for path in g[&#34;inside&#34;]:
                        print(&#34;  &#34;, path)

    def _pred(self, input_data, verbose=False):
        print(&#34;... prediction&#34;)
        goal_dataset = build_goal_dataset(input_data, self.tensor_provider)
        print(&#34;... loaded variables&#34;)
        for key, param in self.comp_expl_graph.state_dict().items():
            print(key, param.shape)
        print(&#34;... initialization&#34;)
        batch_size = self.flags.sgd_minibatch_size
        test_idx = get_goal_dataset(goal_dataset)
        loss_cls = self.loss_cls()
        evaluator = TprismEvaluator(goal_dataset)
        print(&#34;... predicting&#34;)
        start_t = time.time()
        outputs = []
        labels = []
        for j, goal in enumerate(goal_dataset):
            # valid
            num_itr = len(test_idx[j]) // batch_size
            evaluator.start_epoch()
            for itr in range(num_itr):
                self._set_batch_input(goal, test_idx, j, itr)
                goal_inside, loss_list = self.comp_expl_graph.forward()
                loss, output, label = loss_cls.call(
                    self.graph, goal_inside, self.tensor_provider
                )
                evaluator.update(loss[j], loss_list, j)
                _o = output[j].detach().numpy()
                _l = label[j].detach().numpy() if label is not None else None
                metrics = loss_cls.metrics(_o, _l)
                evaluator.update_data(output, label, j)
            ##
            print(evaluator.get_msg(&#34;test&#34;))
            # print(evaluator.output[j],evaluator.label[j])
            metrics = loss_cls.metrics(
                np.array(evaluator.output[j]), np.array(evaluator.label[j])
            )
            outputs.append(np.array(evaluator.output[j]))
            labels.append(evaluator.label[j])
        pred_time = time.time() - start_t
        print(&#34;test time:{0}&#34;.format(pred_time) + &#34;[sec]&#34;)
        return labels, outputs

    def save_draw_graph(self, g, base_name):
        html = draw_graph.show_graph(g)
        fp = open(base_name + &#34;.html&#34;, &#34;w&#34;)
        fp.write(html)
        dot = draw_graph.tf_to_dot(g)
        dot.render(base_name)


def run_preparing(g, sess, args):
    input_data = load_input_data(args.dataset)
    graph, options = load_explanation_graph(args.expl_graph, args.flags)
    flags = Flags(args, options)
    flags.update()
    ##
    loss_loader = LossLoader()
    loss_loader.load_all(&#34;loss/&#34;)
    loss_cls = loss_loader.get_loss(flags.sgd_loss)
    ##
    tensor_provider = torch_expl_graph.TorchSwitchTensorProvider()
    embedding_generators = []
    if flags.embedding:
        eg = embed_gen.DatasetEmbeddingGenerator()
        eg.load(flags.embedding)
        embedding_generators.append(eg)
    if flags.const_embedding:
        eg = embed_gen.ConstEmbeddingGenerator()
        eg.load(flags.const_embedding)
        embedding_generators.append(eg)
    tensor_provider.build(
        graph,
        options,
        input_data,
        flags,
        load_embeddings=False,
        embedding_generators=embedding_generators,
    )


def run_training(args):
    if args.dataset is not None:
        input_data = load_input_data(args.dataset)
    else:
        input_data = None
    graph, options = load_explanation_graph(args.expl_graph, args.flags)
    flags = Flags(args, options)
    flags.update()
    ##
    loss_loader = LossLoader()
    loss_loader.load_all(&#34;loss/torch*&#34;)
    loss_cls = loss_loader.get_loss(flags.sgd_loss)
    ##
    print(&#34;... computational graph&#34;)
    model = TprismModel(flags, options, graph, loss_cls)
    model.build(input_data, load_embeddings=False, embedding_key=&#34;train&#34;)
    start_t = time.time()
    if flags.cycle:
        print(&#34;... fit with cycle&#34;)
        model.solve()
    elif input_data is not None:
        print(&#34;... fit with input data&#34;)
        model.export_computational_graph(input_data)
        print(&#34;=========&#34;)
        model.fit(input_data, verbose=False)
        model.pred(input_data)
    else:
        print(&#34;... fit without input&#34;)
        model.fit()

    train_time = time.time() - start_t
    print(&#34;total training time:{0}&#34;.format(train_time) + &#34;[sec]&#34;)


def run_test(args):
    if args.dataset is not None:
        input_data = load_input_data(args.dataset)
    else:
        input_data = None
    graph, options = load_explanation_graph(args.expl_graph, args.flags)
    flags = Flags(args, options)
    flags.update()
    ##
    loss_loader = LossLoader()
    loss_loader.load_all(&#34;loss/torch*&#34;)
    loss_cls = loss_loader.get_loss(flags.sgd_loss)
    ##
    print(&#34;... computational graph&#34;)
    model = TprismModel(flags, options, graph, loss_cls)
    model.build(input_data, load_embeddings=False, embedding_key=&#34;test&#34;)
    model.load(flags.model + &#34;.best.model&#34;)
    start_t = time.time()
    print(&#34;... prediction&#34;)
    if flags.cycle:
        model.solve(goal_dataset)
    elif input_data is not None:
        pred_y, out = model.pred(input_data)
    else:
        pred_y, out = model.pred()
    train_time = time.time() - start_t
    print(&#34;total training time:{0}&#34;.format(train_time) + &#34;[sec]&#34;)
    print(&#34;... output&#34;)
    np.save(flags.output,out)
    print(&#34;[SAVE]&#34;, flags.output)
    data = {}
    for j, root_obj in enumerate(graph.root_list):
        multi_root = False
        if len(root_obj.roots) &gt; 1:  # preference
            multi_root = True
        for i, el in enumerate(root_obj.roots):
            sid = el.sorted_id
            gg = graph.goals[sid].node
            name = gg.goal.name
            # name=to_string_goal(gg.goal)
            if multi_root:
                data[sid] = {&#34;name&#34;: name, &#34;data&#34;: out[j][i]}
            else:
                data[sid] = {&#34;name&#34;: name, &#34;data&#34;: out[j]}

    fp = open(&#34;output.pkl&#34;, &#34;wb&#34;)
    pickle.dump(data, fp)


def main():
    # set random seed
    seed = 1234
    np.random.seed(seed)

    parser = argparse.ArgumentParser()
    parser.add_argument(&#34;mode&#34;, type=str, help=&#34;train/test&#34;)
    parser.add_argument(&#34;--config&#34;, type=str, default=None, help=&#34;config json file&#34;)

    parser.add_argument(
            &#34;--data&#34;, type=str, default=None, nargs=&#34;+&#34;, help=&#34;[from prolog] data json file (deprecated: use --dataset)&#34;
    )
    parser.add_argument(
            &#34;--dataset&#34;, type=str, default=None, nargs=&#34;+&#34;, help=&#34;[from prolog] dataset file&#34;
    )
    ## intermediate data
    parser.add_argument(
        &#34;--intermediate_data_prefix&#34;,
        type=str,
        default=None,
        help=&#34;intermediate data (deprecated: use --input)&#34;,
    )
    parser.add_argument(
        &#34;--input&#34;,
        &#34;-I&#34;,
        type=str,
        default=None,
        help=&#34;input intermediate data&#34;,
    )
    parser.add_argument(
        &#34;--expl_graph&#34;,
        type=str,
        default=None,
        help=&#34;[from prolog] explanation graph json file&#34;,
    )
    parser.add_argument(
        &#34;--flags&#34;, type=str, default=None, help=&#34;[from prolog] flags json file&#34;
    )
    parser.add_argument(&#34;--model&#34;, type=str, default=None, help=&#34;model file&#34;)
    parser.add_argument(&#34;--vocab&#34;, type=str, default=None, help=&#34;vocabrary file&#34;)
    ##
    parser.add_argument(&#34;--embedding&#34;, type=str, default=None, help=&#34;embedding file&#34;)
    parser.add_argument(&#34;--const_embedding&#34;, type=str, default=None, help=&#34;model file&#34;)
    parser.add_argument(&#34;--draw_graph&#34;, type=str, default=None, help=&#34;graph file&#34;)

    parser.add_argument(
        &#34;--output&#34;, type=str, default=&#34;./output.pkl&#34;, help=&#34;output file&#34;
    )

    parser.add_argument(
        &#34;--gpu&#34;,
        type=str,
        default=None,
        help=&#34;constraint gpus (default: all) (e.g. --gpu 0,2)&#34;,
    )
    parser.add_argument(
        &#34;--cpu&#34;, action=&#34;store_true&#34;, help=&#34;cpu mode (calcuration only with cpu)&#34;
    )

    parser.add_argument(&#34;--no_verb&#34;, action=&#34;store_true&#34;, help=&#34;verb&#34;)

    parser.add_argument(
        &#34;--sgd_minibatch_size&#34;, type=str, default=None, help=&#34;[prolog flag]&#34;
    )
    parser.add_argument(&#34;--max_iterate&#34;, type=str, default=None, help=&#34;[prolog flag]&#34;)
    parser.add_argument(&#34;--epoch&#34;, type=str, default=None, help=&#34;[prolog flag]&#34;)
    parser.add_argument(
        &#34;--sgd_learning_rate&#34;, type=float, default=0.01, help=&#34;[prolog flag]&#34;
    )
    parser.add_argument(
        &#34;--sgd_loss&#34;,
        type=str,
        default=&#34;base_loss&#34;,
        help=&#34;[prolog flag] nll/preference_pair&#34;,
    )
    parser.add_argument(&#34;--sgd_patience&#34;, type=int, default=3, help=&#34;[prolog flag] &#34;)

    parser.add_argument(&#34;--cycle&#34;, action=&#34;store_true&#34;, help=&#34;cycle&#34;)

    args = parser.parse_args()
    # config
    if args.config is None:
        pass
    else:
        print(&#34;[LOAD] &#34;, args.config)
        fp = open(args.config, &#34;r&#34;)
        config.update(json.load(fp))

    # gpu/cpu
    if args.cpu:
        os.environ[&#34;CUDA_VISIBLE_DEVICES&#34;] = &#34;&#34;
    elif args.gpu is not None:
        os.environ[&#34;CUDA_VISIBLE_DEVICES&#34;] = args.gpu

    # deprecated
    if args.data is not None:
        args.dataset=args.data
    if args.intermediate_data_prefix is not None:
        if args.expl_graph is None:
            args.expl_graph = args.intermediate_data_prefix + &#34;expl.json&#34;
        if args.flags is None:
            args.flags = args.intermediate_data_prefix + &#34;flags.json&#34;
        if args.model is None:
            args.model = args.intermediate_data_prefix + &#34;model&#34;
        if args.vocab is None:
            args.vocab = args.intermediate_data_prefix + &#34;vocab.pkl&#34;
    elif args.input is not None:
        # setting default input data
        sep=&#34;.&#34;
        if os.path.isdir(args.input):
            sep=&#34;&#34;
        if args.expl_graph is None:
            args.expl_graph = args.input + sep + &#34;expl.json&#34;
        if args.flags is None:
            args.flags = args.input + sep + &#34;flags.json&#34;
        if args.model is None:
            args.model = args.input + sep + &#34;model&#34;
        if args.vocab is None:
            args.vocab = args.input + sep + &#34;vocab.pkl&#34;
    #
    ##
    # setup
    seed = 1234
    torch.manual_seed(seed)
    # mode
    if args.mode == &#34;train&#34;:
        run_training(args)
    if args.mode == &#34;prepare&#34;:
        run_preparing(args)
    if args.mode == &#34;test&#34; or args.mode == &#34;pred&#34;:
        run_test(args)
    elif args.mode == &#34;cv&#34;:
        run_train_cv(args)
    if args.mode == &#34;show&#34;:
        run_display(args)


if __name__ == &#34;__main__&#34;:
    main()</code></pre>
</details>
</section>
<section>
</section>
<section>
</section>
<section>
<h2 class="section-title" id="header-functions">Functions</h2>
<dl>
<dt id="tprism.torch_tprism.main"><code class="name flex">
<span>def <span class="ident">main</span></span>(<span>)</span>
</code></dt>
<dd>
<div class="desc"></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def main():
    # set random seed
    seed = 1234
    np.random.seed(seed)

    parser = argparse.ArgumentParser()
    parser.add_argument(&#34;mode&#34;, type=str, help=&#34;train/test&#34;)
    parser.add_argument(&#34;--config&#34;, type=str, default=None, help=&#34;config json file&#34;)

    parser.add_argument(
            &#34;--data&#34;, type=str, default=None, nargs=&#34;+&#34;, help=&#34;[from prolog] data json file (deprecated: use --dataset)&#34;
    )
    parser.add_argument(
            &#34;--dataset&#34;, type=str, default=None, nargs=&#34;+&#34;, help=&#34;[from prolog] dataset file&#34;
    )
    ## intermediate data
    parser.add_argument(
        &#34;--intermediate_data_prefix&#34;,
        type=str,
        default=None,
        help=&#34;intermediate data (deprecated: use --input)&#34;,
    )
    parser.add_argument(
        &#34;--input&#34;,
        &#34;-I&#34;,
        type=str,
        default=None,
        help=&#34;input intermediate data&#34;,
    )
    parser.add_argument(
        &#34;--expl_graph&#34;,
        type=str,
        default=None,
        help=&#34;[from prolog] explanation graph json file&#34;,
    )
    parser.add_argument(
        &#34;--flags&#34;, type=str, default=None, help=&#34;[from prolog] flags json file&#34;
    )
    parser.add_argument(&#34;--model&#34;, type=str, default=None, help=&#34;model file&#34;)
    parser.add_argument(&#34;--vocab&#34;, type=str, default=None, help=&#34;vocabrary file&#34;)
    ##
    parser.add_argument(&#34;--embedding&#34;, type=str, default=None, help=&#34;embedding file&#34;)
    parser.add_argument(&#34;--const_embedding&#34;, type=str, default=None, help=&#34;model file&#34;)
    parser.add_argument(&#34;--draw_graph&#34;, type=str, default=None, help=&#34;graph file&#34;)

    parser.add_argument(
        &#34;--output&#34;, type=str, default=&#34;./output.pkl&#34;, help=&#34;output file&#34;
    )

    parser.add_argument(
        &#34;--gpu&#34;,
        type=str,
        default=None,
        help=&#34;constraint gpus (default: all) (e.g. --gpu 0,2)&#34;,
    )
    parser.add_argument(
        &#34;--cpu&#34;, action=&#34;store_true&#34;, help=&#34;cpu mode (calcuration only with cpu)&#34;
    )

    parser.add_argument(&#34;--no_verb&#34;, action=&#34;store_true&#34;, help=&#34;verb&#34;)

    parser.add_argument(
        &#34;--sgd_minibatch_size&#34;, type=str, default=None, help=&#34;[prolog flag]&#34;
    )
    parser.add_argument(&#34;--max_iterate&#34;, type=str, default=None, help=&#34;[prolog flag]&#34;)
    parser.add_argument(&#34;--epoch&#34;, type=str, default=None, help=&#34;[prolog flag]&#34;)
    parser.add_argument(
        &#34;--sgd_learning_rate&#34;, type=float, default=0.01, help=&#34;[prolog flag]&#34;
    )
    parser.add_argument(
        &#34;--sgd_loss&#34;,
        type=str,
        default=&#34;base_loss&#34;,
        help=&#34;[prolog flag] nll/preference_pair&#34;,
    )
    parser.add_argument(&#34;--sgd_patience&#34;, type=int, default=3, help=&#34;[prolog flag] &#34;)

    parser.add_argument(&#34;--cycle&#34;, action=&#34;store_true&#34;, help=&#34;cycle&#34;)

    args = parser.parse_args()
    # config
    if args.config is None:
        pass
    else:
        print(&#34;[LOAD] &#34;, args.config)
        fp = open(args.config, &#34;r&#34;)
        config.update(json.load(fp))

    # gpu/cpu
    if args.cpu:
        os.environ[&#34;CUDA_VISIBLE_DEVICES&#34;] = &#34;&#34;
    elif args.gpu is not None:
        os.environ[&#34;CUDA_VISIBLE_DEVICES&#34;] = args.gpu

    # deprecated
    if args.data is not None:
        args.dataset=args.data
    if args.intermediate_data_prefix is not None:
        if args.expl_graph is None:
            args.expl_graph = args.intermediate_data_prefix + &#34;expl.json&#34;
        if args.flags is None:
            args.flags = args.intermediate_data_prefix + &#34;flags.json&#34;
        if args.model is None:
            args.model = args.intermediate_data_prefix + &#34;model&#34;
        if args.vocab is None:
            args.vocab = args.intermediate_data_prefix + &#34;vocab.pkl&#34;
    elif args.input is not None:
        # setting default input data
        sep=&#34;.&#34;
        if os.path.isdir(args.input):
            sep=&#34;&#34;
        if args.expl_graph is None:
            args.expl_graph = args.input + sep + &#34;expl.json&#34;
        if args.flags is None:
            args.flags = args.input + sep + &#34;flags.json&#34;
        if args.model is None:
            args.model = args.input + sep + &#34;model&#34;
        if args.vocab is None:
            args.vocab = args.input + sep + &#34;vocab.pkl&#34;
    #
    ##
    # setup
    seed = 1234
    torch.manual_seed(seed)
    # mode
    if args.mode == &#34;train&#34;:
        run_training(args)
    if args.mode == &#34;prepare&#34;:
        run_preparing(args)
    if args.mode == &#34;test&#34; or args.mode == &#34;pred&#34;:
        run_test(args)
    elif args.mode == &#34;cv&#34;:
        run_train_cv(args)
    if args.mode == &#34;show&#34;:
        run_display(args)</code></pre>
</details>
</dd>
<dt id="tprism.torch_tprism.run_preparing"><code class="name flex">
<span>def <span class="ident">run_preparing</span></span>(<span>g, sess, args)</span>
</code></dt>
<dd>
<div class="desc"></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def run_preparing(g, sess, args):
    input_data = load_input_data(args.dataset)
    graph, options = load_explanation_graph(args.expl_graph, args.flags)
    flags = Flags(args, options)
    flags.update()
    ##
    loss_loader = LossLoader()
    loss_loader.load_all(&#34;loss/&#34;)
    loss_cls = loss_loader.get_loss(flags.sgd_loss)
    ##
    tensor_provider = torch_expl_graph.TorchSwitchTensorProvider()
    embedding_generators = []
    if flags.embedding:
        eg = embed_gen.DatasetEmbeddingGenerator()
        eg.load(flags.embedding)
        embedding_generators.append(eg)
    if flags.const_embedding:
        eg = embed_gen.ConstEmbeddingGenerator()
        eg.load(flags.const_embedding)
        embedding_generators.append(eg)
    tensor_provider.build(
        graph,
        options,
        input_data,
        flags,
        load_embeddings=False,
        embedding_generators=embedding_generators,
    )</code></pre>
</details>
</dd>
<dt id="tprism.torch_tprism.run_test"><code class="name flex">
<span>def <span class="ident">run_test</span></span>(<span>args)</span>
</code></dt>
<dd>
<div class="desc"></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def run_test(args):
    if args.dataset is not None:
        input_data = load_input_data(args.dataset)
    else:
        input_data = None
    graph, options = load_explanation_graph(args.expl_graph, args.flags)
    flags = Flags(args, options)
    flags.update()
    ##
    loss_loader = LossLoader()
    loss_loader.load_all(&#34;loss/torch*&#34;)
    loss_cls = loss_loader.get_loss(flags.sgd_loss)
    ##
    print(&#34;... computational graph&#34;)
    model = TprismModel(flags, options, graph, loss_cls)
    model.build(input_data, load_embeddings=False, embedding_key=&#34;test&#34;)
    model.load(flags.model + &#34;.best.model&#34;)
    start_t = time.time()
    print(&#34;... prediction&#34;)
    if flags.cycle:
        model.solve(goal_dataset)
    elif input_data is not None:
        pred_y, out = model.pred(input_data)
    else:
        pred_y, out = model.pred()
    train_time = time.time() - start_t
    print(&#34;total training time:{0}&#34;.format(train_time) + &#34;[sec]&#34;)
    print(&#34;... output&#34;)
    np.save(flags.output,out)
    print(&#34;[SAVE]&#34;, flags.output)
    data = {}
    for j, root_obj in enumerate(graph.root_list):
        multi_root = False
        if len(root_obj.roots) &gt; 1:  # preference
            multi_root = True
        for i, el in enumerate(root_obj.roots):
            sid = el.sorted_id
            gg = graph.goals[sid].node
            name = gg.goal.name
            # name=to_string_goal(gg.goal)
            if multi_root:
                data[sid] = {&#34;name&#34;: name, &#34;data&#34;: out[j][i]}
            else:
                data[sid] = {&#34;name&#34;: name, &#34;data&#34;: out[j]}

    fp = open(&#34;output.pkl&#34;, &#34;wb&#34;)
    pickle.dump(data, fp)</code></pre>
</details>
</dd>
<dt id="tprism.torch_tprism.run_training"><code class="name flex">
<span>def <span class="ident">run_training</span></span>(<span>args)</span>
</code></dt>
<dd>
<div class="desc"></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def run_training(args):
    if args.dataset is not None:
        input_data = load_input_data(args.dataset)
    else:
        input_data = None
    graph, options = load_explanation_graph(args.expl_graph, args.flags)
    flags = Flags(args, options)
    flags.update()
    ##
    loss_loader = LossLoader()
    loss_loader.load_all(&#34;loss/torch*&#34;)
    loss_cls = loss_loader.get_loss(flags.sgd_loss)
    ##
    print(&#34;... computational graph&#34;)
    model = TprismModel(flags, options, graph, loss_cls)
    model.build(input_data, load_embeddings=False, embedding_key=&#34;train&#34;)
    start_t = time.time()
    if flags.cycle:
        print(&#34;... fit with cycle&#34;)
        model.solve()
    elif input_data is not None:
        print(&#34;... fit with input data&#34;)
        model.export_computational_graph(input_data)
        print(&#34;=========&#34;)
        model.fit(input_data, verbose=False)
        model.pred(input_data)
    else:
        print(&#34;... fit without input&#34;)
        model.fit()

    train_time = time.time() - start_t
    print(&#34;total training time:{0}&#34;.format(train_time) + &#34;[sec]&#34;)</code></pre>
</details>
</dd>
</dl>
</section>
<section>
<h2 class="section-title" id="header-classes">Classes</h2>
<dl>
<dt id="tprism.torch_tprism.TprismEvaluator"><code class="flex name class">
<span>class <span class="ident">TprismEvaluator</span></span>
<span>(</span><span>goal_dataset=None)</span>
</code></dt>
<dd>
<div class="desc"><p>Evaluator for pytorch system</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">class TprismEvaluator:
    &#34;&#34;&#34;Evaluator for pytorch system

    &#34;&#34;&#34;
    def __init__(self, goal_dataset=None):
        if goal_dataset is not None:
            self.n_goals = len(goal_dataset)
        else:
            self.n_goals = 1
        self.loss_history = [[] for _ in range(self.n_goals)]
        self.loss_dict_history = [[] for _ in range(self.n_goals)]
        self.label = [[] for _ in range(self.n_goals)]
        self.output = [[] for _ in range(self.n_goals)]

    def start_epoch(self):
        self.running_loss = [0.0 for _ in range(self.n_goals)]
        self.running_loss_dict = [{} for _ in range(self.n_goals)]
        self.running_count = [0 for _ in range(self.n_goals)]

    def update(self, loss, loss_dict, j):
        &#34;&#34;&#34;
            This function is called the last of batch iteration
              loss,output, label: return values of forward
              j: goal index of iteration
              num_itr:: number of iterations
        &#34;&#34;&#34;
        self.running_loss[j] += loss
        self.running_count[j] += 1
        for k, v in loss_dict.items():
            if k in self.running_loss_dict:
                self.running_loss_dict[j][k] += v
            else:
                self.running_loss_dict[j][k] = v

    def stop_epoch(self, j=0, mean_flag=True):
        if mean_flag:
            self.running_loss[j] /= self.running_count[j]
            for k in self.running_loss_dict[j].keys():
                self.running_loss_dict[j][k] /= self.running_count[j]
        self.loss_history[j].append(self.running_loss[j])
        self.loss_dict_history[j].append(self.running_loss_dict[j])

    def get_dict(self, prefix=&#34;train&#34;):
        result = {}
        for j in range(self.n_goals):
            key = &#34;{:s}-loss&#34;.format(prefix)
            val = self.running_loss[j]
            result[key] = float(val)
            for k, v in self.running_loss_dict[j].items():
                if k[0] != &#34;*&#34;:
                    m = &#34;{:s}-{:s}-loss&#34;.format(prefix, k)
                else:
                    m = &#34;*{:s}-{:s}&#34;.format(prefix, k[1:])
                result[m] = float(v)
            return result

    def get_msg(self, prefix=&#34;train&#34;):
        msg = []
        for key, val in self.get_dict(prefix=prefix).items():
            m = &#34;{:s}: {:.3f}&#34;.format(key, val)
            msg.append(m)
        return &#34;  &#34;.join(msg)

    def get_loss(self):
        return self.running_loss

    def update_data(self, output, label, j):
        if type(output[j]) == list:  # preference
            _o = [o.detach().numpy() for o in output[j]]
        else:
            _o = output[j].detach().numpy()
        self.output[j].extend(_o)
        if label is not None:
            _l = label[j].detach().numpy()
            self.label[j].extend(_l)</code></pre>
</details>
<h3>Methods</h3>
<dl>
<dt id="tprism.torch_tprism.TprismEvaluator.get_dict"><code class="name flex">
<span>def <span class="ident">get_dict</span></span>(<span>self, prefix='train')</span>
</code></dt>
<dd>
<div class="desc"></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def get_dict(self, prefix=&#34;train&#34;):
    result = {}
    for j in range(self.n_goals):
        key = &#34;{:s}-loss&#34;.format(prefix)
        val = self.running_loss[j]
        result[key] = float(val)
        for k, v in self.running_loss_dict[j].items():
            if k[0] != &#34;*&#34;:
                m = &#34;{:s}-{:s}-loss&#34;.format(prefix, k)
            else:
                m = &#34;*{:s}-{:s}&#34;.format(prefix, k[1:])
            result[m] = float(v)
        return result</code></pre>
</details>
</dd>
<dt id="tprism.torch_tprism.TprismEvaluator.get_loss"><code class="name flex">
<span>def <span class="ident">get_loss</span></span>(<span>self)</span>
</code></dt>
<dd>
<div class="desc"></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def get_loss(self):
    return self.running_loss</code></pre>
</details>
</dd>
<dt id="tprism.torch_tprism.TprismEvaluator.get_msg"><code class="name flex">
<span>def <span class="ident">get_msg</span></span>(<span>self, prefix='train')</span>
</code></dt>
<dd>
<div class="desc"></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def get_msg(self, prefix=&#34;train&#34;):
    msg = []
    for key, val in self.get_dict(prefix=prefix).items():
        m = &#34;{:s}: {:.3f}&#34;.format(key, val)
        msg.append(m)
    return &#34;  &#34;.join(msg)</code></pre>
</details>
</dd>
<dt id="tprism.torch_tprism.TprismEvaluator.start_epoch"><code class="name flex">
<span>def <span class="ident">start_epoch</span></span>(<span>self)</span>
</code></dt>
<dd>
<div class="desc"></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def start_epoch(self):
    self.running_loss = [0.0 for _ in range(self.n_goals)]
    self.running_loss_dict = [{} for _ in range(self.n_goals)]
    self.running_count = [0 for _ in range(self.n_goals)]</code></pre>
</details>
</dd>
<dt id="tprism.torch_tprism.TprismEvaluator.stop_epoch"><code class="name flex">
<span>def <span class="ident">stop_epoch</span></span>(<span>self, j=0, mean_flag=True)</span>
</code></dt>
<dd>
<div class="desc"></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def stop_epoch(self, j=0, mean_flag=True):
    if mean_flag:
        self.running_loss[j] /= self.running_count[j]
        for k in self.running_loss_dict[j].keys():
            self.running_loss_dict[j][k] /= self.running_count[j]
    self.loss_history[j].append(self.running_loss[j])
    self.loss_dict_history[j].append(self.running_loss_dict[j])</code></pre>
</details>
</dd>
<dt id="tprism.torch_tprism.TprismEvaluator.update"><code class="name flex">
<span>def <span class="ident">update</span></span>(<span>self, loss, loss_dict, j)</span>
</code></dt>
<dd>
<div class="desc"><p>This function is called the last of batch iteration
loss,output, label: return values of forward
j: goal index of iteration
num_itr:: number of iterations</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def update(self, loss, loss_dict, j):
    &#34;&#34;&#34;
        This function is called the last of batch iteration
          loss,output, label: return values of forward
          j: goal index of iteration
          num_itr:: number of iterations
    &#34;&#34;&#34;
    self.running_loss[j] += loss
    self.running_count[j] += 1
    for k, v in loss_dict.items():
        if k in self.running_loss_dict:
            self.running_loss_dict[j][k] += v
        else:
            self.running_loss_dict[j][k] = v</code></pre>
</details>
</dd>
<dt id="tprism.torch_tprism.TprismEvaluator.update_data"><code class="name flex">
<span>def <span class="ident">update_data</span></span>(<span>self, output, label, j)</span>
</code></dt>
<dd>
<div class="desc"></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def update_data(self, output, label, j):
    if type(output[j]) == list:  # preference
        _o = [o.detach().numpy() for o in output[j]]
    else:
        _o = output[j].detach().numpy()
    self.output[j].extend(_o)
    if label is not None:
        _l = label[j].detach().numpy()
        self.label[j].extend(_l)</code></pre>
</details>
</dd>
</dl>
</dd>
<dt id="tprism.torch_tprism.TprismModel"><code class="flex name class">
<span>class <span class="ident">TprismModel</span></span>
<span>(</span><span>flags, options, graph, loss_cls)</span>
</code></dt>
<dd>
<div class="desc"><p>T-PRISM model for pytorch</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">class TprismModel:
    &#34;&#34;&#34;T-PRISM model for pytorch

    &#34;&#34;&#34;
 
    def __init__(self, flags, options, graph, loss_cls):
        self.graph = graph
        self.flags = flags
        self.options = options
        self.loss_cls = loss_cls

    def build(self, input_data, load_embeddings, embedding_key):
        self._build_embedding(embedding_key)
        self._set_data(input_data, load_embeddings)
        self._build_explanation_graph()

    def _build_embedding(self, embedding_key):
        embedding_generators = []
        if self.flags.embedding:
            eg = embed_gen.DatasetEmbeddingGenerator()
            eg.load(self.flags.embedding, key=embedding_key)
            embedding_generators.append(eg)
        if self.flags.const_embedding:
            eg = embed_gen.ConstEmbeddingGenerator()
            eg.load(self.flags.const_embedding)
            embedding_generators.append(eg)
        cycle_embedding_generator = None
        if self.flags.cycle:
            cycle_embedding_generator = embed_gen.CycleEmbeddingGenerator()
            cycle_embedding_generator.load(self.options)
            embedding_generators.append(cycle_embedding_generator)
        self.embedding_generators = embedding_generators
        self.cycle_embedding_generator = cycle_embedding_generator

    def _set_data(self, input_data, load_embeddings):
        self.tensor_provider = torch_expl_graph.TorchSwitchTensorProvider()
        self.tensor_provider.build(
            self.graph,
            self.options,
            input_data,
            self.flags,
            load_embeddings=load_embeddings,
            embedding_generators=self.embedding_generators,
        )

    def _build_explanation_graph(self):
        self.comp_expl_graph = torch_expl_graph.TorchComputationalExplGraph(
            self.graph, self.tensor_provider, self.cycle_embedding_generator
        )

    def solve(self, input_data=None):
        if input_data is None:
            self._solve_no_data()

    def _solve_no_data(self):
        print(&#34;... training phase&#34;)
        print(&#34;... training variables&#34;)
        for key, param in self.comp_expl_graph.state_dict().items():
            print(key, param.shape)
        print(&#34;... building explanation graph&#34;)
        prev_loss = None
        for step in range(self.flags.max_iterate):
            feed_dict = {}
            for embedding_generator in self.embedding_generators:
                if embedding_generator is not None:
                    feed_dict = embedding_generator.build_feed(feed_dict, None)
            self.tensor_provider.set_input(feed_dict)
            # out_inside = self.sess.run(inside, feed_dict=feed_dict)
            goal_inside, loss_list = self.comp_expl_graph.forward(verbose=True)
            inside = []
            for goal in goal_inside:
                l1 = goal[&#34;inside&#34;]
                inside.append(l1)
            loss = 0
            for embedding_generator in self.embedding_generators:
                if embedding_generator is not None:
                    loss = embedding_generator.update(inside)
            print(&#34;step&#34;, step, &#34;loss:&#34;, loss)
            if loss &lt; 1.0e-20:
                break
            if prev_loss is not None and not loss &lt; prev_loss:
                pass
            prev_loss = loss
        ##

    def fit(self, input_data=None, verbose=False):
        if input_data is None:
            return self._fit_no_data(verbose)
        else:
            return self._fit(input_data, verbose)

    def _fit_no_data(self, verbose):
        print(&#34;... training phase&#34;)
        print(&#34;... training variables&#34;)
        for key, param in self.comp_expl_graph.state_dict().items():
            print(key, param.shape)
        optimizer = optim.Adam(
            self.comp_expl_graph.parameters(),
            self.flags.sgd_learning_rate,
            weight_decay=1.0e-10,
        )
        print(&#34;... initialization&#34;)
        loss_cls = self.loss_cls()
        print(&#34;... building explanation graph&#34;)

        best_total_loss = None
        train_evaluator = TprismEvaluator()
        for epoch in range(self.flags.max_iterate):
            start_t = time.time()
            # train
            feed_dict = {}
            for embedding_generator in self.embedding_generators:
                if embedding_generator is not None:
                    feed_dict = embedding_generator.build_feed(feed_dict, None)
            self.tensor_provider.set_input(feed_dict)
            # print(&#34;... iteration&#34;)
            goal_inside, loss_list = self.comp_expl_graph.forward()
            loss, output, label = loss_cls.call(
                self.graph, goal_inside, self.tensor_provider
            )
            optimizer.zero_grad()
            total_loss = torch.sum(loss, dim=0)
            total_loss.backward()
            optimizer.step()
            metrics = loss_cls.metrics(output, label)
            metrics.update(loss_list)
            train_evaluator.start_epoch()
            train_evaluator.update(total_loss, metrics, 0)
            train_evaluator.stop_epoch()
            # display_graph(output[j],&#39;graph_pytorch&#39;)

            # train_acc=sklearn.metrics.accuracy_score(all_label,all_output)
            train_time = time.time() - start_t
            print(&#34;[{:4d}] &#34;.format(epoch + 1), train_evaluator.get_msg(&#34;train&#34;))
            print(&#34;train time:{0}&#34;.format(train_time) + &#34;[sec]&#34;)
            if (
                best_total_loss is None
                or train_evaluator.running_loss[0] &lt; best_total_loss
            ):
                best_total_loss = train_evaluator.running_loss[0]
                self.save(self.flags.model + &#34;.best.model&#34;)
        self.save(self.flags.model + &#34;.last.model&#34;)

    def _build_feed(self, ph_vars, dataset, idx, verbose=False):
        if verbose:
            for i, ph in enumerate(ph_vars):
                print(&#34;[INFO feed]&#34;, ph, ph.name)
                print(&#34;[INFO feed]&#34;, dataset[i, idx].shape)
        feed_dict = {ph: torch.tensor(dataset[i, idx]) for i, ph in enumerate(ph_vars)}
        return feed_dict

    def _set_batch_input(self, goal, train_idx, j, itr):
        batch_size = self.flags.sgd_minibatch_size
        ph_vars = goal[&#34;placeholders&#34;]
        dataset = goal[&#34;dataset&#34;]
        batch_idx = train_idx[j][itr * batch_size : (itr + 1) * batch_size]
        feed_dict = self._build_feed(ph_vars, dataset, batch_idx)
        # for k,v in feed_dict.items():
        #    print(k,v.shape)
        for embedding_generator in self.embedding_generators:
            if embedding_generator is not None:
                feed_dict = embedding_generator.build_feed(feed_dict, batch_idx)
        self.tensor_provider.set_input(feed_dict)

    def _fit(self, input_data, verbose):
        print(&#34;... training phase&#34;)
        goal_dataset = build_goal_dataset(input_data, self.tensor_provider)
        print(&#34;... training variables&#34;)
        for key, param in self.comp_expl_graph.state_dict().items():
            print(key, param.shape)
        optimizer = optim.Adam(
            self.comp_expl_graph.parameters(),
            self.flags.sgd_learning_rate,
            weight_decay=1.0e-10,
        )
        best_valid_loss = [None for _ in range(len(goal_dataset))]
        patient_count = 0
        batch_size = self.flags.sgd_minibatch_size
        print(&#34;... splitting data&#34;)
        train_idx, valid_idx = split_goal_dataset(goal_dataset)
        loss_cls = self.loss_cls()
        print(&#34;... starting training&#34;)
        for epoch in range(self.flags.max_iterate):
            start_t = time.time()
            train_evaluator = TprismEvaluator(goal_dataset)
            valid_evaluator = TprismEvaluator(goal_dataset)
            for j, goal in enumerate(goal_dataset):
                if verbose:
                    print(goal)
                np.random.shuffle(train_idx[j])
                # training update
                num_itr = len(train_idx[j]) // batch_size
                train_evaluator.start_epoch()
                for itr in range(num_itr):
                    self._set_batch_input(goal, train_idx, j, itr)
                    goal_inside, loss_list = self.comp_expl_graph.forward(
                        verbose=verbose
                    )
                    loss, output, label = loss_cls.call(
                        self.graph, goal_inside, self.tensor_provider
                    )
                    if label is not None:
                        metrics = loss_cls.metrics(
                            output[j].detach().numpy(), label[j].detach().numpy()
                        )
                    else:
                        metrics = loss_cls.metrics(output[j].detach().numpy(), None)
                    loss_list.update(metrics)
                    # display_graph(output[j],&#39;graph_pytorch&#39;)
                    optimizer.zero_grad()
                    loss[j].backward()
                    optimizer.step()
                    train_evaluator.update(loss[j], loss_list, j)
                # validation
                num_itr = len(valid_idx[j]) // batch_size
                valid_evaluator.start_epoch()
                for itr in range(num_itr):
                    self._set_batch_input(goal, valid_idx, j, itr)
                    goal_inside, loss_list = self.comp_expl_graph.forward()
                    loss, output, label = loss_cls.call(
                        self.graph, goal_inside, self.tensor_provider
                    )
                    if label is not None:
                        metrics = loss_cls.metrics(
                            output[j].detach().numpy(), label[j].detach().numpy()
                        )
                    else:
                        metrics = loss_cls.metrics(output[j].detach().numpy(), None)
                    loss_list.update(metrics)
                    valid_evaluator.update(loss[j], loss_list, j)
                # checking validation loss for early stopping
                if (
                    best_valid_loss[j] is None
                    or best_valid_loss[j] &gt; valid_evaluator.running_loss[j]
                ):
                    best_valid_loss[j] = valid_evaluator.running_loss[j]
                    patient_count = 0
                    check_point_flag = True
                    self.save(self.flags.model + &#34;.best.model&#34;)
                else:
                    patient_count += 1
                ckpt_msg = &#34;*&#34; if check_point_flag else &#34;&#34;
                print(
                    &#34;[{:4d}] &#34;.format(epoch + 1),
                    train_evaluator.get_msg(&#34;train&#34;),
                    valid_evaluator.get_msg(&#34;valid&#34;),
                    &#34;({:2d})&#34;.format(patient_count),
                    ckpt_msg,
                )
                if patient_count == self.flags.sgd_patience:
                    break
            train_time = time.time() - start_t
            print(&#34;train time:{0}&#34;.format(train_time) + &#34;[sec]&#34;)
        self.save(self.flags.model + &#34;.last.model&#34;)

    def save(self, filename):
        torch.save(self.comp_expl_graph.state_dict(), filename)

    def load(self, filename):
        self.comp_expl_graph.load_state_dict(torch.load(filename))

    def pred(self, input_data=None, verbose=False):
        if input_data is not None:
            return self._pred(input_data, verbose)
        else:
            return self._pred_no_data(verbose)

    def _pred_no_data(self, verbose):
        print(&#34;... prediction&#34;)
        print(&#34;... loaded variables&#34;)
        for key, param in self.comp_expl_graph.state_dict().items():
            print(key, param.shape)
        print(&#34;... initialization&#34;)
        loss_cls = self.loss_cls()
        evaluator = TprismEvaluator()
        ###
        feed_dict = {}
        for embedding_generator in self.embedding_generators:
            if embedding_generator is not None:
                feed_dict = embedding_generator.build_feed(feed_dict, None)
        self.tensor_provider.set_input(feed_dict)
        print(&#34;... predicting&#34;)
        start_t = time.time()
        goal_inside, loss_list = self.comp_expl_graph.forward()
        loss, output, label = loss_cls.call(
            self.graph, goal_inside, self.tensor_provider
        )
        metrics=loss_cls.metrics(output, label)
        # train_acc=sklearn.metrics.accuracy_score(all_label,all_output)
        #print(&#34;loss:&#34;, np.sum(evaluator.get_loss())
        print(&#34;loss:&#34;, torch.sum(loss))
        print(&#34;metrics:&#34;, metrics)
        pred_time = time.time() - start_t
        print(&#34;prediction time:{0}&#34;.format(pred_time) + &#34;[sec]&#34;)
        if label is None:
            label=label.detach().numpy()
        output=output.detach().numpy()
        return label, output

    def export_computational_graph(self, input_data, verbose=False):
        print(&#34;... prediction&#34;)
        goal_dataset = build_goal_dataset(input_data, self.tensor_provider)
        print(&#34;... loaded variables&#34;)
        for key, param in self.comp_expl_graph.state_dict().items():
            print(key, param.shape)
        print(&#34;... initialization&#34;)
        batch_size = self.flags.sgd_minibatch_size
        test_idx = get_goal_dataset(goal_dataset)
        loss_cls = self.loss_cls()
        evaluator = TprismEvaluator(goal_dataset)
        print(&#34;... predicting&#34;)
        start_t = time.time()
        outputs = []
        labels = []
        for j, goal in enumerate(goal_dataset):
            # valid
            num_itr = len(test_idx[j]) // batch_size
            evaluator.start_epoch()
            for itr in range(num_itr):
                self._set_batch_input(goal, test_idx, j, itr)
                goal_inside, loss_list = self.comp_expl_graph.forward(dryrun=True)
                for g in goal_inside:
                    print(g)
                    for path in g[&#34;inside&#34;]:
                        print(&#34;  &#34;, path)

    def _pred(self, input_data, verbose=False):
        print(&#34;... prediction&#34;)
        goal_dataset = build_goal_dataset(input_data, self.tensor_provider)
        print(&#34;... loaded variables&#34;)
        for key, param in self.comp_expl_graph.state_dict().items():
            print(key, param.shape)
        print(&#34;... initialization&#34;)
        batch_size = self.flags.sgd_minibatch_size
        test_idx = get_goal_dataset(goal_dataset)
        loss_cls = self.loss_cls()
        evaluator = TprismEvaluator(goal_dataset)
        print(&#34;... predicting&#34;)
        start_t = time.time()
        outputs = []
        labels = []
        for j, goal in enumerate(goal_dataset):
            # valid
            num_itr = len(test_idx[j]) // batch_size
            evaluator.start_epoch()
            for itr in range(num_itr):
                self._set_batch_input(goal, test_idx, j, itr)
                goal_inside, loss_list = self.comp_expl_graph.forward()
                loss, output, label = loss_cls.call(
                    self.graph, goal_inside, self.tensor_provider
                )
                evaluator.update(loss[j], loss_list, j)
                _o = output[j].detach().numpy()
                _l = label[j].detach().numpy() if label is not None else None
                metrics = loss_cls.metrics(_o, _l)
                evaluator.update_data(output, label, j)
            ##
            print(evaluator.get_msg(&#34;test&#34;))
            # print(evaluator.output[j],evaluator.label[j])
            metrics = loss_cls.metrics(
                np.array(evaluator.output[j]), np.array(evaluator.label[j])
            )
            outputs.append(np.array(evaluator.output[j]))
            labels.append(evaluator.label[j])
        pred_time = time.time() - start_t
        print(&#34;test time:{0}&#34;.format(pred_time) + &#34;[sec]&#34;)
        return labels, outputs

    def save_draw_graph(self, g, base_name):
        html = draw_graph.show_graph(g)
        fp = open(base_name + &#34;.html&#34;, &#34;w&#34;)
        fp.write(html)
        dot = draw_graph.tf_to_dot(g)
        dot.render(base_name)</code></pre>
</details>
<h3>Methods</h3>
<dl>
<dt id="tprism.torch_tprism.TprismModel.build"><code class="name flex">
<span>def <span class="ident">build</span></span>(<span>self, input_data, load_embeddings, embedding_key)</span>
</code></dt>
<dd>
<div class="desc"></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def build(self, input_data, load_embeddings, embedding_key):
    self._build_embedding(embedding_key)
    self._set_data(input_data, load_embeddings)
    self._build_explanation_graph()</code></pre>
</details>
</dd>
<dt id="tprism.torch_tprism.TprismModel.export_computational_graph"><code class="name flex">
<span>def <span class="ident">export_computational_graph</span></span>(<span>self, input_data, verbose=False)</span>
</code></dt>
<dd>
<div class="desc"></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def export_computational_graph(self, input_data, verbose=False):
    print(&#34;... prediction&#34;)
    goal_dataset = build_goal_dataset(input_data, self.tensor_provider)
    print(&#34;... loaded variables&#34;)
    for key, param in self.comp_expl_graph.state_dict().items():
        print(key, param.shape)
    print(&#34;... initialization&#34;)
    batch_size = self.flags.sgd_minibatch_size
    test_idx = get_goal_dataset(goal_dataset)
    loss_cls = self.loss_cls()
    evaluator = TprismEvaluator(goal_dataset)
    print(&#34;... predicting&#34;)
    start_t = time.time()
    outputs = []
    labels = []
    for j, goal in enumerate(goal_dataset):
        # valid
        num_itr = len(test_idx[j]) // batch_size
        evaluator.start_epoch()
        for itr in range(num_itr):
            self._set_batch_input(goal, test_idx, j, itr)
            goal_inside, loss_list = self.comp_expl_graph.forward(dryrun=True)
            for g in goal_inside:
                print(g)
                for path in g[&#34;inside&#34;]:
                    print(&#34;  &#34;, path)</code></pre>
</details>
</dd>
<dt id="tprism.torch_tprism.TprismModel.fit"><code class="name flex">
<span>def <span class="ident">fit</span></span>(<span>self, input_data=None, verbose=False)</span>
</code></dt>
<dd>
<div class="desc"></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def fit(self, input_data=None, verbose=False):
    if input_data is None:
        return self._fit_no_data(verbose)
    else:
        return self._fit(input_data, verbose)</code></pre>
</details>
</dd>
<dt id="tprism.torch_tprism.TprismModel.load"><code class="name flex">
<span>def <span class="ident">load</span></span>(<span>self, filename)</span>
</code></dt>
<dd>
<div class="desc"></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def load(self, filename):
    self.comp_expl_graph.load_state_dict(torch.load(filename))</code></pre>
</details>
</dd>
<dt id="tprism.torch_tprism.TprismModel.pred"><code class="name flex">
<span>def <span class="ident">pred</span></span>(<span>self, input_data=None, verbose=False)</span>
</code></dt>
<dd>
<div class="desc"></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def pred(self, input_data=None, verbose=False):
    if input_data is not None:
        return self._pred(input_data, verbose)
    else:
        return self._pred_no_data(verbose)</code></pre>
</details>
</dd>
<dt id="tprism.torch_tprism.TprismModel.save"><code class="name flex">
<span>def <span class="ident">save</span></span>(<span>self, filename)</span>
</code></dt>
<dd>
<div class="desc"></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def save(self, filename):
    torch.save(self.comp_expl_graph.state_dict(), filename)</code></pre>
</details>
</dd>
<dt id="tprism.torch_tprism.TprismModel.save_draw_graph"><code class="name flex">
<span>def <span class="ident">save_draw_graph</span></span>(<span>self, g, base_name)</span>
</code></dt>
<dd>
<div class="desc"></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def save_draw_graph(self, g, base_name):
    html = draw_graph.show_graph(g)
    fp = open(base_name + &#34;.html&#34;, &#34;w&#34;)
    fp.write(html)
    dot = draw_graph.tf_to_dot(g)
    dot.render(base_name)</code></pre>
</details>
</dd>
<dt id="tprism.torch_tprism.TprismModel.solve"><code class="name flex">
<span>def <span class="ident">solve</span></span>(<span>self, input_data=None)</span>
</code></dt>
<dd>
<div class="desc"></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def solve(self, input_data=None):
    if input_data is None:
        self._solve_no_data()</code></pre>
</details>
</dd>
</dl>
</dd>
</dl>
</section>
</article>
<nav id="sidebar">
<h1>Index</h1>
<div class="toc">
<ul></ul>
</div>
<ul id="index">
<li><h3>Super-module</h3>
<ul>
<li><code><a title="tprism" href="index.html">tprism</a></code></li>
</ul>
</li>
<li><h3><a href="#header-functions">Functions</a></h3>
<ul class="">
<li><code><a title="tprism.torch_tprism.main" href="#tprism.torch_tprism.main">main</a></code></li>
<li><code><a title="tprism.torch_tprism.run_preparing" href="#tprism.torch_tprism.run_preparing">run_preparing</a></code></li>
<li><code><a title="tprism.torch_tprism.run_test" href="#tprism.torch_tprism.run_test">run_test</a></code></li>
<li><code><a title="tprism.torch_tprism.run_training" href="#tprism.torch_tprism.run_training">run_training</a></code></li>
</ul>
</li>
<li><h3><a href="#header-classes">Classes</a></h3>
<ul>
<li>
<h4><code><a title="tprism.torch_tprism.TprismEvaluator" href="#tprism.torch_tprism.TprismEvaluator">TprismEvaluator</a></code></h4>
<ul class="two-column">
<li><code><a title="tprism.torch_tprism.TprismEvaluator.get_dict" href="#tprism.torch_tprism.TprismEvaluator.get_dict">get_dict</a></code></li>
<li><code><a title="tprism.torch_tprism.TprismEvaluator.get_loss" href="#tprism.torch_tprism.TprismEvaluator.get_loss">get_loss</a></code></li>
<li><code><a title="tprism.torch_tprism.TprismEvaluator.get_msg" href="#tprism.torch_tprism.TprismEvaluator.get_msg">get_msg</a></code></li>
<li><code><a title="tprism.torch_tprism.TprismEvaluator.start_epoch" href="#tprism.torch_tprism.TprismEvaluator.start_epoch">start_epoch</a></code></li>
<li><code><a title="tprism.torch_tprism.TprismEvaluator.stop_epoch" href="#tprism.torch_tprism.TprismEvaluator.stop_epoch">stop_epoch</a></code></li>
<li><code><a title="tprism.torch_tprism.TprismEvaluator.update" href="#tprism.torch_tprism.TprismEvaluator.update">update</a></code></li>
<li><code><a title="tprism.torch_tprism.TprismEvaluator.update_data" href="#tprism.torch_tprism.TprismEvaluator.update_data">update_data</a></code></li>
</ul>
</li>
<li>
<h4><code><a title="tprism.torch_tprism.TprismModel" href="#tprism.torch_tprism.TprismModel">TprismModel</a></code></h4>
<ul class="">
<li><code><a title="tprism.torch_tprism.TprismModel.build" href="#tprism.torch_tprism.TprismModel.build">build</a></code></li>
<li><code><a title="tprism.torch_tprism.TprismModel.export_computational_graph" href="#tprism.torch_tprism.TprismModel.export_computational_graph">export_computational_graph</a></code></li>
<li><code><a title="tprism.torch_tprism.TprismModel.fit" href="#tprism.torch_tprism.TprismModel.fit">fit</a></code></li>
<li><code><a title="tprism.torch_tprism.TprismModel.load" href="#tprism.torch_tprism.TprismModel.load">load</a></code></li>
<li><code><a title="tprism.torch_tprism.TprismModel.pred" href="#tprism.torch_tprism.TprismModel.pred">pred</a></code></li>
<li><code><a title="tprism.torch_tprism.TprismModel.save" href="#tprism.torch_tprism.TprismModel.save">save</a></code></li>
<li><code><a title="tprism.torch_tprism.TprismModel.save_draw_graph" href="#tprism.torch_tprism.TprismModel.save_draw_graph">save_draw_graph</a></code></li>
<li><code><a title="tprism.torch_tprism.TprismModel.solve" href="#tprism.torch_tprism.TprismModel.solve">solve</a></code></li>
</ul>
</li>
</ul>
</li>
</ul>
</nav>
</main>
<footer id="footer">
<p>Generated by <a href="https://pdoc3.github.io/pdoc" title="pdoc: Python API documentation generator"><cite>pdoc</cite> 0.10.0</a>.</p>
</footer>
</body>
</html>